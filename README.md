# TLM

The [Trustworthy Language Model (TLM)](https://cleanlab.ai/blog/trustworthy-language-model/) scores the **trustworthiness** of outputs from *any* LLM in *real-time*.

Automatically detect hallucinated/incorrect responses in: Q&A (RAG), Chatbots, Agents, Structured Outputs, Data Extraction, Tool Calling, Classification/Tagging, Data Labeling, and other LLM applications.

Use TLM to: guardrail AI mistakes before they are served to user, escalate cases where AI is untrustworthy to humans, discover incorrect LLM (or human) generated outputs in datasets/logs, or boost AI accuracy.

Powered by *uncertainty estimation* techniques, TLM **works out of the box**, and does **not** require: data preparation/labeling work, custom model training/serving infrastructure, time investment in customization.

Learn more and see precision/recall benchmarks with frontier models (from OpenAI, Anthropic, Google, etc): [Blog](https://cleanlab.ai/blog/), [Research Paper](https://aclanthology.org/2024.acl-long.283/)

## Usage

See [notebooks](notebooks) for Jupyter notebooks with example usage.
