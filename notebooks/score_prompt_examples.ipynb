{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efb474-6182-4168-912d-d4c693d2e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e /Users/Kelsey/code/tlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acd706-14c6-45a6-973d-5a5486249471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file inside of tlm project directory with variables filled in\n",
    "# Note that AWS variables are only required if calling Bedrock models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bada3b-b8d7-4c9e-aa7e-0bad15dfb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_FILE_PATH = \"/Users/Kelsey/code/tlm/.env\"\n",
    "\n",
    "load_dotenv(ENV_FILE_PATH, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3d843-44a1-4a76-b307-a32326ec60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Question-Answering workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7ad5b-aaa4-4b30-99e4-0184afb5d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "\n",
    "openai_args = { \"messages\": [ { \"role\": \"user\", \"content\": \"how many R's in strawberry\" } ] }\n",
    "result = await inference(openai_args=openai_args)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccab6ef-6ad5-4f6a-a2bb-172a6e9bc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA workflow with custom evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5459ed-be10-4704-9acb-5c1bd8f50246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "\n",
    "openai_args = { \"messages\": [ { \"role\": \"user\", \"content\": \"Explain machine learning\" } ] }\n",
    "result = await inference(\n",
    "    openai_args=openai_args,\n",
    "    config_input=ConfigInput(\n",
    "        criteria={\n",
    "            \"clarity\": \"The response is clear and easy to understand.\",\n",
    "            \"conciseness\": \"The response is concise and to the point.\",\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9396a-d60c-49a8-8e8b-0a17dceedaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrain outputs QA workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45686965-1577-4817-baa4-7498dd917ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "from tlm.config.presets import QualityPreset, ReasoningEffort\n",
    "\n",
    "openai_args = { \"messages\": [ { \"role\": \"user\", \"content\": \"Classify the tone of the Text as positive, negative, or neutral. Text: The Earth is a beautiful place.\" } ] }\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=openai_args,\n",
    "    config_input=ConfigInput(\n",
    "        quality_preset=QualityPreset.HIGH,\n",
    "        reasoning_effort=ReasoningEffort.MEDIUM,\n",
    "        constrain_outputs=[\"positive\", \"neutral\", \"negative\"],\n",
    "    ),\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1d2d1-5576-445c-825d-298aa7ac6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82dcb2-8a80-40cb-8d49-1e46fe609971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "\n",
    "openai_args = { \"messages\": [ \n",
    "    {\"role\": \"system\", \"content\": \"Answer the User Prompt using the Context: Simple water bottle holds 28 oz.\" }, \n",
    "    {\"role\": \"user\", \"content\": \"how much water can i put in simple water bottle\" },\n",
    "] }\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=openai_args,\n",
    "    context=\"Simple water bottle holds 28 oz.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f1d75-4df9-493f-8f7d-5ccebc032280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured outputs scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc62d6b-748c-4fe0-a08f-0991e20ab408",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e0451-b194-4030-9fff-71b3a7c6d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.presets import QualityPreset\n",
    "from tlm.config.base import ConfigInput\n",
    "from openai import OpenAI\n",
    "\n",
    "json_schema = {\n",
    "    \"name\": \"calendar_event\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": { \"type\": \"string\" },\n",
    "            \"date\": { \"type\": \"string\" },\n",
    "            \"participants\": { \n",
    "                \"type\": \"array\",\n",
    "                \"items\": { \"type\": \"string\" },\n",
    "            },\n",
    "        },\n",
    "        \"additionalProperties\": False,\n",
    "        \"required\": [ \"name\", \"date\", \"participants\" ],\n",
    "    },\n",
    "}\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": json_schema,\n",
    "}\n",
    "\n",
    "openai_kwargs = {\n",
    "    \"model\": \"gpt-4.1-mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    \"response_format\": response_format,\n",
    "}\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.parse(**openai_kwargs)\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=openai_kwargs,\n",
    "    response={\n",
    "        \"chat_completion\": completion.model_dump(),\n",
    "        \"perplexity\": 0.95,\n",
    "    },\n",
    "    config_input=ConfigInput(\n",
    "        quality_preset=QualityPreset.HIGH,\n",
    "        model=\"gpt-4.1-mini\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35b252-4f8f-45bd-b439-b3eba8baff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TLM response using AWS Bedrock model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483b6f5-9655-472c-9430-38112232a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "from tlm.config.presets import QualityPreset\n",
    "\n",
    "openai_kwargs = {\n",
    "    \"model\": \"claude-3.7-sonnet\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"what is the capital of france\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=openai_kwargs,\n",
    "    config_input=ConfigInput(\n",
    "        quality_preset=QualityPreset.HIGH,\n",
    "    ),\n",
    ")\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20254028-113d-4184-b189-a11c1a1eb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TLM response using Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e7a31-f71c-442b-9d88-fcf8a8626a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "from tlm.config.presets import QualityPreset\n",
    "\n",
    "chat_kwargs = {\n",
    "    \"model\": \"gemini/gemini-flash-latest\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"what is the capital of france\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=chat_kwargs,\n",
    "    config_input=ConfigInput(\n",
    "        quality_preset=QualityPreset.HIGH,\n",
    "    ),\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cec26-a6d0-49fe-9cea-a3b1a05e29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TLM response using Deepseek model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319edb27-d326-4e90-8e02-4e4215761e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "from tlm.config.presets import QualityPreset\n",
    "\n",
    "chat_kwargs = {\n",
    "    \"model\": \"deepseek/deepseek-chat\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"name the 3rd month of the year alphabetically\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "result = await inference(\n",
    "    openai_args=chat_kwargs,\n",
    "    config_input=ConfigInput(\n",
    "        quality_preset=QualityPreset.HIGH,\n",
    "    ),\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb9657-3883-4dce-959a-fc6a2e9dc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TLM response using Vertex AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f3bd6-53b5-4396-916e-625c69caf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlm.api import inference\n",
    "from tlm.config.base import ConfigInput\n",
    "from tlm.config.presets import QualityPreset\n",
    "\n",
    "import os\n",
    "import json\n",
    "file_path = '/Users/Kelsey/vertex.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    vertex_credentials = json.load(file)\n",
    "vertex_credentials_json = json.dumps(vertex_credentials)\n",
    "\n",
    "openai_kwargs = {\n",
    "    \"model\": \"vertex_ai/gemini-2.5-pro\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"name the 3rd month alphabetically\"},\n",
    "    ],\n",
    "    \"vertex_credentials\": vertex_credentials_json\n",
    "}\n",
    "result = await inference(\n",
    "    openai_args=openai_kwargs,\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
